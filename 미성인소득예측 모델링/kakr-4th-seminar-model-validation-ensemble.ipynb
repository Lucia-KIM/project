{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) CSV 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('~/Downloads/kakr-4th-competition/train.csv')\n",
    "label = train['income']\n",
    "\n",
    "del train['income']\n",
    "\n",
    "test = pd.read_csv('~/Downloads/kakr-4th-competition/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 값 인코딩\n",
    "label = label.map(lambda x: 1 if x == '>50K' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID 컬럼은 행의 식별자로 필요 없는 컬럼이므로 삭제하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['id']\n",
    "del test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train = train.copy()\n",
    "tmp_test  = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 데이터 확인\n",
    ".head(), .describe(), .info() 등의 함수로 데이터를 확인합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 결측치 처리\n",
    "이전 태진님 강의에서 'workclass', 'occupation', 'native_country' 컬럼에 결측치가 있다는 것을 알 수 있었습니다. <br>\n",
    "일반적인 결측치와 다르게 '?'로 표현되어있는 값들은 해당 컬럼의 최빈값으로 결측치 처리를 진행하겠습니다. <br>\n",
    "\n",
    "##### 범주형 변수의 경우 가장 간단하게 최빈값으로 결측치 처리를 할 수 있지만, 다른 컬럼을 필터링해서 결측치 처리를 할 수 있습니다. ex) education_num 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_na_columns = ['workclass', 'occupation', 'native_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in has_na_columns:\n",
    "    tmp_train.loc[train[c] == '?', c] = train[c].mode()[0]\n",
    "    tmp_test.loc[test[c]   == '?', c] = test[c].mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Log 변환\n",
    "capital_gain 변수와 capital_loss 변수의 분포가 한쪽으로 치우친 형태이므로 Log 변환을 통해 분포의 형태를 조정해주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train['capital_gain'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train['log_capital_gain'] = train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "tmp_test['log_capital_gain']  = test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "\n",
    "tmp_train['log_capital_gain'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['capital_loss'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train['log_capital_loss'] = train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "tmp_test['log_capital_loss'] = test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "\n",
    "tmp_train['log_capital_loss'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train = tmp_train.drop(columns=['capital_loss', 'capital_gain'])\n",
    "tmp_test  = tmp_test.drop(columns=['capital_loss', 'capital_gain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) 데이터 쪼개기\n",
    "##### 1. Train, Valid, Test Set\n",
    "* Train Data : 모델을 학습하는데 사용하는 데이터 (모델이 알고 있는 학습할 데이터, 과거 데이터)\n",
    "* Valid Data : 학습한 모델의 성능을 검증하는 데이터 (모델이 모르는 학습하지 않을 데이터, 모델 검증에 사용하는 데이터, 과거 데이터)\n",
    "* Test Data : 학습한 모델로 예측할 데이터 (모델이 모르는 예측할 데이터, 미래 데이터)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 데이터 쪼개기, Train -> (Train, Valid)\n",
    "- train_test_split 파라미터 \n",
    "    - test_size  (float): Valid(test)의 크기의 비율을 지정\n",
    "    - random_state (int): 데이터를 쪼갤 때 내부적으로 사용되는 난수 값 (해당 값을 지정하지 않으면 매번 달라집니다.)\n",
    "    - shuffle     (bool): 데이터를 쪼갤 때 섞을지 유무\n",
    "    - stratify   (array): Stratify란, 쪼개기 이전의 클래스 비율을 쪼개고 나서도 유지하기 위해 설정해야하는 값입니다. 클래스 라벨을 넣어주면 됩니다.\n",
    "        - ex) 원본 Train 데이터의 클래스 비율이 (7:3) 이었다면, 쪼개어진 Train, Valid(test) 데이터의 클래스 비율도 (7:3)이 됩니다. 당연히 분류 데이터에서만 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tmp_train, tmp_valid, y_train, y_valid = train_test_split(tmp_train, label, \n",
    "                                                          test_size=0.3,\n",
    "                                                          random_state=2020,\n",
    "                                                          shuffle=True,\n",
    "                                                          stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 초기화\n",
    "tmp_train = tmp_train.reset_index(drop=True)\n",
    "tmp_valid = tmp_valid.reset_index(drop=True)\n",
    "tmp_test  = tmp_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) 스케일링\n",
    "Scikit-learn 라이브러리에 있는 Standard Scaler를 사용해서 수치형 변수들의 표준화를 진행하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [c for c, t in zip(tmp_train.dtypes.index, tmp_train.dtypes) if t == 'O'] \n",
    "num_columns = [c for c in tmp_train.columns if c not in cat_columns]\n",
    "\n",
    "print('범주형 변수: \\n{}\\n\\n 수치형 변수: \\n{}\\n'.format(cat_columns, num_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "tmp_train[num_columns] = scaler.fit_transform(tmp_train[num_columns])\n",
    "tmp_valid[num_columns] = scaler.transform(tmp_valid[num_columns])\n",
    "tmp_test[num_columns]  = scaler.transform(tmp_test[num_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) 인코딩\n",
    "범주형 변수를 수치형 변수로 인코딩 하겠습니다. 범주형 변수에는 Onehot Encoding을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "tmp_all = pd.concat([tmp_train, tmp_valid, tmp_test])\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ohe.fit(tmp_all[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_columns = list()\n",
    "for lst in ohe.categories_:\n",
    "    ohe_columns += lst.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_cat = pd.DataFrame(ohe.transform(tmp_train[cat_columns]), columns=ohe_columns)\n",
    "new_valid_cat = pd.DataFrame(ohe.transform(tmp_valid[cat_columns]), columns=ohe_columns)\n",
    "new_test_cat  = pd.DataFrame(ohe.transform(tmp_test[cat_columns]), columns=ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train = pd.concat([tmp_train, new_train_cat], axis=1)\n",
    "tmp_valid = pd.concat([tmp_valid, new_valid_cat], axis=1)\n",
    "tmp_test = pd.concat([tmp_test, new_test_cat], axis=1)\n",
    "\n",
    "# 기존 범주형 변수 제거\n",
    "tmp_train = tmp_train.drop(columns=cat_columns)\n",
    "tmp_valid = tmp_valid.drop(columns=cat_columns)\n",
    "tmp_test = tmp_test.drop(columns=cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_y_train = y_train\n",
    "tmp_y_valid = y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scikit-Learn 분류 모델 사용해보기\n",
    "Scikit-Learn의 기본 분류 모델을 사용해보겠습니다. <br>\n",
    "각 모델의 평가 메트릭은 대회 평가 메트릭인 f1_score를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 로지스틱 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(tmp_train, tmp_y_train)\n",
    "\n",
    "y_pred = lr.predict(tmp_valid)\n",
    "\n",
    "print(f\"Logistic Regression F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 서포트 벡터 머신(rbf 커널)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "svc.fit(tmp_train, tmp_y_train)\n",
    "\n",
    "y_pred = svc.predict(tmp_valid)\n",
    "\n",
    "print(f\"Support Vector Machine F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(tmp_train, tmp_y_train)\n",
    "\n",
    "y_pred = rf.predict(tmp_valid)\n",
    "\n",
    "print(f\"RandomForest F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(tree_method='gpu_hist')\n",
    "\n",
    "xgb.fit(tmp_train, tmp_y_train)\n",
    "\n",
    "y_pred = xgb.predict(tmp_valid)\n",
    "\n",
    "print(f\"XGBoost F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier(tree_method='gpu_hist')\n",
    "\n",
    "lgb.fit(tmp_train, tmp_y_train)\n",
    "\n",
    "y_pred = lgb.predict(tmp_valid)\n",
    "\n",
    "print(f\"LightGBM F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. k-Fold Cross Validation\n",
    "먼저 1. 에서 정리한 전처리 프로세스를 하나의 함수로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x_train, x_valid, x_test):\n",
    "    tmp_x_train = x_train.copy()\n",
    "    tmp_x_valid = x_valid.copy()\n",
    "    tmp_x_test  = x_test.copy()\n",
    "    \n",
    "    tmp_x_train = tmp_x_train.reset_index(drop=True)\n",
    "    tmp_x_valid = tmp_x_valid.reset_index(drop=True)\n",
    "    tmp_x_test  = tmp_x_test.reset_index(drop=True)\n",
    "    \n",
    "    tmp_x_train.loc[tmp_x_train[c] == '?', c] = tmp_x_train[c].mode()[0]\n",
    "    tmp_x_valid.loc[tmp_x_valid[c] == '?', c] = tmp_x_valid[c].mode()[0]\n",
    "    tmp_x_test.loc[tmp_x_test[c]   == '?', c] = tmp_x_test[c].mode()[0]\n",
    "    \n",
    "    tmp_x_train['log_capital_loss'] = tmp_x_train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "    tmp_x_valid['log_capital_loss'] = tmp_x_valid['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "    tmp_x_test['log_capital_loss'] = tmp_x_test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "    \n",
    "    tmp_x_train['log_capital_gain'] = tmp_x_train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "    tmp_x_valid['log_capital_gain'] = tmp_x_valid['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "    tmp_x_test['log_capital_gain'] = tmp_x_test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "    \n",
    "    tmp_x_train = tmp_x_train.drop(columns=['capital_loss', 'capital_gain'])\n",
    "    tmp_x_valid = tmp_x_valid.drop(columns=['capital_loss', 'capital_gain'])\n",
    "    tmp_x_test  = tmp_x_test.drop(columns=['capital_loss', 'capital_gain'])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    tmp_x_train[num_columns] = scaler.fit_transform(tmp_x_train[num_columns])\n",
    "    tmp_x_valid[num_columns] = scaler.transform(tmp_x_valid[num_columns])\n",
    "    tmp_x_test[num_columns]  = scaler.transform(tmp_x_test[num_columns])\n",
    "    \n",
    "    tmp_all = pd.concat([tmp_x_train, tmp_x_valid, tmp_x_test])\n",
    "\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    ohe.fit(tmp_all[cat_columns])\n",
    "    \n",
    "    ohe_columns = list()\n",
    "    for lst in ohe.categories_:\n",
    "        ohe_columns += lst.tolist()\n",
    "    \n",
    "    tmp_train_cat = pd.DataFrame(ohe.transform(tmp_x_train[cat_columns]), columns=ohe_columns)\n",
    "    tmp_valid_cat = pd.DataFrame(ohe.transform(tmp_x_valid[cat_columns]), columns=ohe_columns)\n",
    "    tmp_test_cat  = pd.DataFrame(ohe.transform(tmp_x_test[cat_columns]), columns=ohe_columns)\n",
    "    \n",
    "    tmp_x_train = pd.concat([tmp_x_train, tmp_train_cat], axis=1)\n",
    "    tmp_x_valid = pd.concat([tmp_x_valid, tmp_valid_cat], axis=1)\n",
    "    tmp_x_test = pd.concat([tmp_x_test, tmp_test_cat], axis=1)\n",
    "\n",
    "    tmp_x_train = tmp_x_train.drop(columns=cat_columns)\n",
    "    tmp_x_valid = tmp_x_valid.drop(columns=cat_columns)\n",
    "    tmp_x_test = tmp_x_test.drop(columns=cat_columns)\n",
    "    \n",
    "    return tmp_x_train.values, tmp_x_valid.values, tmp_x_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_f1(y, t, threshold=0.5):\n",
    "    t = t.get_label()\n",
    "    y_bin = (y > threshold).astype(int) \n",
    "    return 'f1',f1_score(t, y_bin, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = list()\n",
    "oof_pred = np.zeros((test.shape[0],))\n",
    "\n",
    "for i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n",
    "    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n",
    "    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n",
    "    \n",
    "    # 전처리\n",
    "    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n",
    "    \n",
    "    # 모델 정의\n",
    "    clf = XGBClassifier(tree_method='gpu_hist')\n",
    "    \n",
    "    # 모델 학습\n",
    "    clf.fit(x_train, y_train,\n",
    "            eval_set = [[x_valid, y_valid]], \n",
    "            eval_metric = xgb_f1,        \n",
    "            early_stopping_rounds = 100,\n",
    "            verbose = 100,  )\n",
    "\n",
    "    # 훈련, 검증 데이터 Log Loss 확인\n",
    "    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n",
    "    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n",
    "    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n",
    "    \n",
    "    val_scores.append(val_f1_score)\n",
    "    \n",
    "\n",
    "# 교차 검증 F1 Score 평균 계산하기\n",
    "print('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. OOF(Out-Of-Fold) 앙상블\n",
    "k-Fold를 활용해서 모델 검증 및 각 폴드의 결과를 앙상블하는 OOF 앙상블 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = list()\n",
    "oof_pred = np.zeros((test.shape[0], ))\n",
    "\n",
    "for i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n",
    "    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n",
    "    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n",
    "    \n",
    "    # 전처리\n",
    "    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n",
    "    \n",
    "    # 모델 정의\n",
    "    clf = XGBClassifier(tree_method='gpu_hist')\n",
    "    \n",
    "    # 모델 학습\n",
    "    clf.fit(x_train, y_train,\n",
    "            eval_set = [[x_valid, y_valid]], \n",
    "            eval_metric = xgb_f1,        \n",
    "            early_stopping_rounds = 100,\n",
    "            verbose = 100,  )\n",
    "\n",
    "    # 훈련, 검증 데이터 F1 Score 확인\n",
    "    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n",
    "    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n",
    "    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n",
    "    \n",
    "    val_scores.append(val_f1_score)\n",
    "    \n",
    "    oof_pred += clf.predict_proba(x_test)[:, 1] / n_splits\n",
    "    \n",
    "\n",
    "# 교차 검증 F1 Score 평균 계산하기\n",
    "print('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stacking 앙상블\n",
    "2 stage 앙상블인 Stacking 앙상블 입니다. Stacking 앙상블은 수십개의 1 stage 모델의 결과를 모아 2 stage 모델로 학습 후 결과를 내는 앙상블 방식입니다.\n",
    "\n",
    "#### 1) 1 stage 결과 모으기\n",
    "Stacking 앙상블을 진행할 1 stage 모델의 결과(train, test)를 모읍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = list()\n",
    "\n",
    "new_x_train_list = [np.zeros((train.shape[0], 1)) for _ in range(4)]\n",
    "new_x_test_list  = [np.zeros((test.shape[0], 1)) for _ in range(4)]\n",
    "\n",
    "for i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n",
    "    print(f\"Fold {i} Start\")\n",
    "    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n",
    "    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n",
    "    \n",
    "    # 전처리\n",
    "    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n",
    "    \n",
    "    # 모델 정의\n",
    "    clfs = [LogisticRegression(), \n",
    "            RandomForestClassifier(), \n",
    "            XGBClassifier(tree_method='gpu_hist'), \n",
    "            LGBMClassifier(tree_method='gpu_hist')]\n",
    "    \n",
    "    for model_idx, clf in enumerate(clfs):\n",
    "        clf.fit(x_train, y_train)\n",
    "        \n",
    "        new_x_train_list[model_idx][val_idx, :] = clf.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n",
    "        new_x_test_list[model_idx][:] += clf.predict_proba(x_test)[:, 1].reshape(-1, 1) / n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = pd.DataFrame(np.concatenate(new_x_train_list, axis=1), columns=None)\n",
    "new_label = np.concatenate([tmp_y_train, tmp_y_valid])\n",
    "new_test = pd.DataFrame(np.concatenate(new_x_test_list, axis=1), columns=None)\n",
    "\n",
    "new_train.shape, new_label.shape, new_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 2 Stage Meta Model 학습\n",
    "new_train, new_test에 들어있는 변수는 모두 수치형 변수이므로 Standard Scaling만 진행하겠습니다.<br>\n",
    "새로 생성한 데이터 new_train, new_test 데이터를 가지고 2 Stage Meta Model을 학습하고 결과를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = list()\n",
    "oof_pred = np.zeros((test.shape[0], ))\n",
    "\n",
    "for i, (trn_idx, val_idx) in enumerate(skf.split(new_train, new_label)):\n",
    "    x_train, y_train = new_train.iloc[trn_idx, :], new_label[trn_idx]\n",
    "    x_valid, y_valid = new_train.iloc[val_idx, :], new_label[val_idx]\n",
    "    \n",
    "    # 전처리\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_valid = scaler.transform(x_valid)\n",
    "    x_test  = scaler.transform(new_test)\n",
    "    \n",
    "    # 모델 정의\n",
    "    clf = XGBClassifier(tree_method='gpu_hist')\n",
    "    \n",
    "    # 모델 학습\n",
    "    clf.fit(x_train, y_train,\n",
    "            eval_set = [[x_valid, y_valid]], \n",
    "            eval_metric = xgb_f1,        \n",
    "            early_stopping_rounds = 100,\n",
    "            verbose = 100,  )\n",
    "\n",
    "    # 훈련, 검증 데이터 F1 Score 확인\n",
    "    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n",
    "    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n",
    "    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n",
    "    \n",
    "    val_scores.append(val_f1_score)\n",
    "    \n",
    "#     oof_pred += clf.predict_proba(x_test)[:, 1] / n_splits\n",
    "    \n",
    "\n",
    "# 교차 검증 F1 Score 평균 계산하기\n",
    "print('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쌓은 모델이 적어서 성능이 좋지 않으니 OOF 앙상블로 예측한 값을 결과로 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 결과 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"/kaggle/input/kakr-4th-competition/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.loc[:, 'prediction'] = (oof_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
