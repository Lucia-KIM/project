{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import missingno as msno\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "train = pd.read_csv('~/Downloads/kakr-4th-competition/train.csv')\n",
    "test = pd.read_csv('~/Downloads/kakr-4th-competition/test.csv')\n",
    "sample_submission = pd.read_csv('~/Downloads/kakr-4th-competition/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) column 제거\n",
    "def col_reduction(df):\n",
    "    df.drop(['id','fnlwgt','education','relationship','native_country','workclass'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2) marital_status 조정\n",
    "def mar_st(df):\n",
    "    df['marital_status'] = (df['marital_status'] == 'Married-civ-spouse').astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 3) capital_gain, loss 조정\n",
    "def capital(df):\n",
    "    df['cap_gain_high'] = (df['capital_gain'] != 0).astype(int)\n",
    "    df['cap_loss_high'] = (df['capital_loss'] >= 1700).astype(int)\n",
    "    df['capital_gain'] = df['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 4) age 조정 함수\n",
    "def age(df):\n",
    "    df.loc[df['age'] < 20, 'age_range'] = '~20'\n",
    "    df.loc[df['age'] >= 65, 'age_range'] = '~65'\n",
    "\n",
    "    down = 20\n",
    "    for i in range(45//5):\n",
    "        df.loc[(df['age'] >= down) & (df['age'] < down+5), 'age_range'] = str(down)+'~'+str(down+5)\n",
    "        down += 5\n",
    "\n",
    "    df['age'] = df['age_range']\n",
    "    df.drop(['age_range'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "# 5) One-hot encoding은 만들지 않았다.\n",
    "\n",
    "# 6) edu_num 새 변수 만들기\n",
    "def edu(df):\n",
    "    df['edu_num_high'] = (df['education_num'] >= 13).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 7) hpw 새 변수 만들기\n",
    "    \n",
    "def hpw(df):\n",
    "    df['hpw_high'] = (df['hours_per_week'] >= 50).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 8) MinMaxScaler\n",
    "def mm_feature(df, feature):\n",
    "    mm_scaler = MinMaxScaler()\n",
    "    \n",
    "    df[feature] = mm_scaler.fit_transform(df[feature].values.reshape(-1,1))\n",
    "    \n",
    "    return df, mm_scaler\n",
    "\n",
    "# 9) target 분리: train은 하고, test는 안하므로 따로 만들겠다.\n",
    "def target_handle(df):\n",
    "    df['income'] = df['income_>50K']\n",
    "    df.drop(['income_>50K','income_<=50K'], axis=1, inplace=True)\n",
    "    \n",
    "    y_df = df.income\n",
    "    X_df = df.drop(['income'], axis=1, inplace=False)\n",
    "    \n",
    "    return X_df, y_df\n",
    "\n",
    "def main(df):\n",
    "    \n",
    "    df1 = col_reduction(df)\n",
    "    df2 = mar_st(df1)\n",
    "    df3 = capital(df2)\n",
    "    df4 = age(df3)\n",
    "    \n",
    "    df5 = pd.get_dummies(df4)\n",
    "    \n",
    "    df6 = edu(df5)\n",
    "    df_fin = hpw(df6)\n",
    "    \n",
    "    return df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용\n",
    "## main: 1) ~ 7)\n",
    "train = main(train)\n",
    "X_test = main(test)\n",
    "\n",
    "## 8) minmax scaler\n",
    "train, mm_scaler1 = mm_feature(train,'education_num')\n",
    "train, mm_scaler2 = mm_feature(train,'hours_per_week')\n",
    "\n",
    "X_test['education_num'] = mm_scaler1.transform(X_test['education_num'].values.reshape(-1,1))\n",
    "X_test['hours_per_week'] = mm_scaler2.transform(X_test['hours_per_week'].values.reshape(-1,1))\n",
    "\n",
    "## 9) X, y split\n",
    "X_train, y_train = target_handle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 기존과 동일\n",
    "# 학습용 데이터 분할처리 (8:2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X_train, y_train,\n",
    "                                                     test_size=.2,\n",
    "                                                     random_state = 42,\n",
    "                                                     shuffle=True,\n",
    "                                                     stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20839, 42)\n",
      "(20839,)\n",
      "==================================================\n",
      "(5210, 42)\n",
      "(5210,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print('='*50)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML 모델 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "              validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost 모델 사용\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(x_train, y_train)\n",
    "y_pred = xgb_model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-F1 Score:  0.87063339731286\n",
      "-Accuracy score:  0.8706333973128599\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1 = f1_score(y_valid, y_pred, average='micro')\n",
    "print('-F1 Score: ', f1)\n",
    "# print(f\"XGBClassifier\\n -F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")\n",
    "\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print('-Accuracy score: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost 알고리즘의 개념 이해\n",
    "XGBoost는 Gradient Boosting 알고리즘을 분산환경에서도 실행할 수 있도록 구현해놓은 라이브러리이다. \n",
    "\n",
    "즉, 앙상블 부스팅(ensemble boosting)의 특징인 가중치 부여를 경사하강법(gradient descent)으로 한다\n",
    "\n",
    "* xgboost의 특징\n",
    "\n",
    "    - gbm보다는 빠르다. (gbm보다 빠른 것입니다.)\n",
    "    - 과적합(overfitting) 방지가 가능한 규제가 포함되어 있다.\n",
    "    - CART(Classification And Regression Tree)를 기반으로 한다. 즉, 분류와 회귀가 둘 다 가능하다\n",
    "    - 조기 종료(early stopping)을 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost의 하이퍼파라미터(xgboost hyperparameter)\n",
    "https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "    \n",
    "- n_estimators(혹은 num_boost_round) : 결정 트리의 개수\n",
    "- max_depth : 트리의 깊이\n",
    "- colsample_bytree : 컬럼의 샘플링 비율(random forest의 max_features와 비슷)\n",
    "- subsample : weak learner가 학습에 사용하는 데이터 샘플링 비율\n",
    "- learning_rete : 학습률\n",
    "- min_split_loss :  리프 노드를 추가적으로 나눌지 결정하는 값\n",
    "- reg_lambda : L2 규제\n",
    "- reg_alpha : L1 규제"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 파라미터 튜닝 (GridSearchCV)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "param_grid={'max_depth':range(3,10,3), 'min_child_weight':range(1,6,2)}\n",
    "\n",
    "# param_grid={'max_depth':range(3,10,3), 'min_child_weight':range(1,6,2), 'gamma':[i/10.0 for i in range(0,5)], \n",
    "#             'subsample':[i/10.0 for i in range(6,10)], 'colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    "#             'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]}\n",
    "\n",
    "grid_sv = GridSearchCV(estimator=xgb.XGBClassifier(learning_rate =0.1, n_estimators=1000, \n",
    "                        objective= 'binary:logistic'), \n",
    "                       param_grid=param_grid, scoring='neg_mean_squared_error')\n",
    "grid_sv.fit(x_train, y_train )\n",
    "print(\"Best 파라미터 :\", grid_sv.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "params = grid_sv.best_params_\n",
    "\n",
    "model = xgb.XGBClassifier(**params)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, reg.predict(X_test))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = xgb.XGBClassifier(\n",
    "    learning_rate =0.05,\n",
    "    n_estimators=1000,\n",
    "    max_depth=8,\n",
    "    min_child_weight=3,\n",
    "    gamma=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    scale_pos_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8698656429942418\n"
     ]
    }
   ],
   "source": [
    "xgb1.fit(x_train, y_train)\n",
    "\n",
    "y_pred1 = xgb1.predict(x_valid)\n",
    "f1 = f1_score(y_valid, y_pred1, average='micro')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = xgb.XGBClassifier(booster='gbtree', \n",
    "    learning_rate =0.05,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    gamma=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    n_jobs=3,\n",
    "    scale_pos_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8700575815738963\n"
     ]
    }
   ],
   "source": [
    "xgb2.fit(x_train, y_train)\n",
    "\n",
    "y_pred1 = xgb2.predict(x_valid)\n",
    "f1 = f1_score(y_valid, y_pred1, average='micro')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM 적용\n",
    "https://nurilee.com/lightgbm-definition-parameter-tuning/\n",
    "    \n",
    "Light GBM은 Gradient Boosting 프레임워크로 Tree 기반 학습 알고리즘으로 Tree가 수직적으로 확장(leaf-wise)되는 방식이다.\n",
    "확장하기 위해서 max delta loss를 가진 leaf를 선택하게 되는데, \n",
    "동일한 leaf를 확장할 때, leaf-wise 알고리즘은 level-wise 알고리즘(수평적 확장)보다 더 많은 loss, 손실을 줄일 수 있다.\n",
    "LGBM은 또한 GPU 학습을 지원하기 때문에 속도가 빠르다.\n",
    "\n",
    "단, Light GBM은 overfitting (과적합)에 민감하고 작은 데이터에 대해서 과적합하기 쉽기 때문에 적은 데이터에는 사용하지 않는 것을 권한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM 파라미터\n",
    "\n",
    "https://lightgbm.readthedocs.io/en/latest/Parameters.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 더 빠른 속도\n",
    "    - bagging_fraction\n",
    "    - max_bin은 작게\n",
    "    - save_binary를 쓰면 데이터 로딩속도가 빨라짐\n",
    "    - parallel learning 사용\n",
    "\n",
    "* 더 높은 정확도\n",
    "    - max_bin을 크게\n",
    "    - num_iterations 는 크게하고 learning_rate는 작게\n",
    "    - num_leaves를 크게(과적합의 원인이 될 수 있음)\n",
    "    - boosting 알고리즘 'dart' 사용\n",
    "\n",
    "* 과적합을 줄이기\n",
    "    - max_bin을 작게\n",
    "    - num_leaves를 작게\n",
    "    - min_data_in_leaf와 min_sum_hessian_in_leaf 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(n_estimators=400)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb_model = LGBMClassifier(n_estimators=400)\n",
    "lgb_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-F1 Score:  0.8664107485604606\n"
     ]
    }
   ],
   "source": [
    "pred = lgb_model.predict(x_valid)\n",
    "\n",
    "f1 = f1_score(y_valid, pred, average='micro')\n",
    "print('-F1 Score: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5044, number of negative: 15795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 324\n",
      "[LightGBM] [Info] Number of data points in the train set: 20839, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242046 -> initscore=-1.141494\n",
      "[LightGBM] [Info] Start training from score -1.141494\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.483689\n",
      "[200]\tvalid_0's binary_logloss: 0.438894\n",
      "[300]\tvalid_0's binary_logloss: 0.406864\n",
      "[400]\tvalid_0's binary_logloss: 0.384137\n",
      "[500]\tvalid_0's binary_logloss: 0.367395\n",
      "[600]\tvalid_0's binary_logloss: 0.35422\n",
      "[700]\tvalid_0's binary_logloss: 0.343499\n",
      "[800]\tvalid_0's binary_logloss: 0.335146\n",
      "[900]\tvalid_0's binary_logloss: 0.328165\n",
      "[1000]\tvalid_0's binary_logloss: 0.322308\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.322308\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_test = lgb.Dataset(x_valid, label=y_valid)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.003\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 10\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 16\n",
    "params['is_training_metric'] = True\n",
    "clf = lgb.train(params, d_train, 1000, d_test, verbose_eval=100, early_stopping_rounds=100)\n",
    "\n",
    "y_pred = clf.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = f1_score(y_valid, y_pred, average='micro')\n",
    "# print('-F1 Score: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
